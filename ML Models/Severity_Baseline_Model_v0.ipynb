{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "solid-mexican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n",
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_validate\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rolled-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"E:/Wasif/PDMotorFeatureExtraction/\"\n",
    "dataset_file = \"tas2_severity_dataset.pkl\"\n",
    "\n",
    "SEED = 1234\n",
    "#TRAIN_TEST_SPLIT = [0.80, 0.20]\n",
    "TRAIN_TEST_DEV_SPLIT = [0.75, 0.15, 0.10]\n",
    "np.random.seed(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "connected-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    dataset = pd.read_pickle(os.path.join(DATA_DIR, dataset_file))\n",
    "    dataset = dataset[(~dataset[\"Right\"].isna()) & (~dataset[\"Left\"].isna())]\n",
    "    dataset[\"rating\"] = dataset[[\"Left\", \"Right\"]].max(axis=1)\n",
    "    \n",
    "    #Duplicate a data point for 4 rating 3 times == to do oversampling, remove later\n",
    "    #temp = dataset[dataset[\"rating\"]==4.0]\n",
    "    #dataset = dataset.append([temp]*3,ignore_index=True)\n",
    "    \n",
    "    X_t = dataset[[\"frequency_components\"]].values\n",
    "    X = np.zeros((X_t.shape[0], X_t[0][0].shape[0]))\n",
    "    i = 0\n",
    "    for x in X_t:\n",
    "        X[i] = x[0]\n",
    "        i +=1\n",
    "    y = dataset[\"rating\"].to_numpy()\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occasional-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMOTE_oversample(X,y):\n",
    "    oversample = SMOTE(k_neighbors=3)\n",
    "    X, y = oversample.fit_resample(X, y)\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "occasional-professional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 128)\n",
      "(179,)\n"
     ]
    }
   ],
   "source": [
    "(X, y) = load_dataset()\n",
    "#(X, y) = SMOTE_oversample(X,y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disciplinary-beach",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_property(X, y):\n",
    "    class0 = np.sum(y==0.0)\n",
    "    class1 = np.sum(y==1.0)\n",
    "    class2 = np.sum(y==2.0)\n",
    "    class3 = np.sum(y==3.0)\n",
    "    class4 = np.sum(y==4.0)\n",
    "    print(class0, class1, class2, class3, class4)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "silent-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, y_train, model):\n",
    "    #(X,y) = SMOTE_oversample(X_train,y_train)\n",
    "    (X, y) = (X_train, y_train)\n",
    "    clf = make_pipeline(StandardScaler(), model)\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "improved-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y, pred_y):\n",
    "    performance = {}\n",
    "    performance['keys'] = ['accuracy', 'auc-roc', 'f1', 'precision', 'recall', 'balanced_accuracy']\n",
    "    performance['accuracy'] = accuracy_score(y, pred_y)\n",
    "    performance['auc-roc'] = roc_auc_score(y, pred_y)\n",
    "    performance['f1'] = f1_score(y, pred_y)\n",
    "    performance['precision'] = precision_score(y, pred_y)\n",
    "    performance['recall'] = recall_score(y, pred_y)\n",
    "    performance['balanced_accuracy'] = balanced_accuracy_score(y, pred_y)\n",
    "    \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "responsible-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split():\n",
    "    (X, y, X_index) = load_dataset()\n",
    "    N = X.shape[0]\n",
    "    N_train = (int)(N*TRAIN_TEST_SPLIT[0])\n",
    "    N_test = N - N_train\n",
    "    \n",
    "    #print(N, N_train, N_test)\n",
    "    \n",
    "    test_indices = np.random.choice(N, N_test, replace=False)\n",
    "    train_indices = [i for i in range(0,N) if i not in test_indices]\n",
    "    \n",
    "    (X_train, y_train) = (X[train_indices,:], y[train_indices])\n",
    "    (X_test, y_test) = (X[test_indices,:], y[test_indices])\n",
    "    \n",
    "    X_index_train = []\n",
    "    X_index_test = []\n",
    "    \n",
    "    for i in train_indices:\n",
    "        X_index_train.append(X_index[i])\n",
    "        \n",
    "    for i in test_indices:\n",
    "        X_index_test.append(X_index[i])\n",
    "    \n",
    "    return (X_train, y_train, X_index_train, X_test, y_test, X_index_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crucial-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exotic-morocco",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_dev_split():\n",
    "    (X, y) = load_dataset()\n",
    "    N = X.shape[0]\n",
    "    N_train = (int)(N*TRAIN_TEST_DEV_SPLIT[0])\n",
    "    N_test = (int)(N*TRAIN_TEST_DEV_SPLIT[1])\n",
    "    N_dev = N - N_train - N_test\n",
    "    \n",
    "    print(\"Train Test Dev\")\n",
    "    print(N_train, N_test, N_dev)\n",
    "    \n",
    "    test_dev_indices = np.random.choice(N, (N_test+N_dev), replace=False)\n",
    "    \n",
    "    test_indices = test_dev_indices[0:N_test]\n",
    "    dev_indices = test_dev_indices[N_test:]\n",
    "    train_indices = [i for i in range(0,N) if i not in test_dev_indices]\n",
    "    \n",
    "    (X_train, y_train) = (X[train_indices,:], y[train_indices])\n",
    "    (X_test, y_test) = (X[test_indices,:], y[test_indices])\n",
    "    (X_dev, y_dev) = (X[dev_indices,:], y[dev_indices])\n",
    "    \n",
    "    return (X_train, y_train, X_test, y_test, X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cathedral-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_dev_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "raised-invite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Test Dev\n",
      "134 26 19\n",
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "#(X, y) = load_dataset()\n",
    "#assert(X.shape[0]==y.shape[0])\n",
    "\n",
    "(X_train, y_train, X_test, y_test, X_dev, y_dev) = train_test_dev_split()\n",
    "assert(X_train.shape[0]==y_train.shape[0])\n",
    "print(\"Dataset Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "prompt-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf 100\n",
      "Dev set MSE, MAE\n",
      "0.5504768314647414 0.5684983634118775\n",
      "Train set MSE, MAE\n",
      "0.44935004002319223 0.42283377164292646\n",
      "\n",
      "===========\n",
      "\n",
      "rbf 50\n",
      "Dev set MSE, MAE\n",
      "0.5383674989470537 0.5761106909191419\n",
      "Train set MSE, MAE\n",
      "0.5202169910319906 0.4707130361821292\n",
      "\n",
      "===========\n",
      "\n",
      "rbf 25\n",
      "Dev set MSE, MAE\n",
      "0.5536945538117554 0.5871787941948763\n",
      "Train set MSE, MAE\n",
      "0.582333649172706 0.5022230657202832\n",
      "\n",
      "===========\n",
      "\n",
      "rbf 10\n",
      "Dev set MSE, MAE\n",
      "0.525198237779151 0.565160300402474\n",
      "Train set MSE, MAE\n",
      "0.6564761691674198 0.5339158546300166\n",
      "\n",
      "===========\n",
      "\n",
      "rbf 1\n",
      "Dev set MSE, MAE\n",
      "0.4952096787095395 0.5564175808703012\n",
      "Train set MSE, MAE\n",
      "0.7655232548193227 0.5785747977554212\n",
      "\n",
      "===========\n",
      "\n",
      "rbf 0.1\n",
      "Dev set MSE, MAE\n",
      "0.475164054308165 0.5416276825772883\n",
      "Train set MSE, MAE\n",
      "0.826657016291105 0.6059255083158177\n",
      "\n",
      "===========\n",
      "\n",
      "poly 100\n",
      "Dev set MSE, MAE\n",
      "0.8900923640009782 0.6704935430485444\n",
      "Train set MSE, MAE\n",
      "0.5835647934366182 0.4660948965421785\n",
      "\n",
      "===========\n",
      "\n",
      "poly 50\n",
      "Dev set MSE, MAE\n",
      "0.7236437859313254 0.6209390693419087\n",
      "Train set MSE, MAE\n",
      "0.6334471623458839 0.48246763724298697\n",
      "\n",
      "===========\n",
      "\n",
      "poly 25\n",
      "Dev set MSE, MAE\n",
      "0.7063645642810623 0.6322905437256238\n",
      "Train set MSE, MAE\n",
      "0.6747230834102513 0.4989569968375368\n",
      "\n",
      "===========\n",
      "\n",
      "poly 10\n",
      "Dev set MSE, MAE\n",
      "0.5194932783535385 0.5360212521833801\n",
      "Train set MSE, MAE\n",
      "0.7352269107345293 0.5291701214170766\n",
      "\n",
      "===========\n",
      "\n",
      "poly 1\n",
      "Dev set MSE, MAE\n",
      "0.48080191578944265 0.533176155952229\n",
      "Train set MSE, MAE\n",
      "0.8338179815007154 0.5877590701193367\n",
      "\n",
      "===========\n",
      "\n",
      "poly 0.1\n",
      "Dev set MSE, MAE\n",
      "0.47806591105579477 0.5315982083412392\n",
      "Train set MSE, MAE\n",
      "0.8643215520652061 0.6039750004858373\n",
      "\n",
      "===========\n",
      "\n",
      "Best Hyperparameters\n",
      "('rbf', 0.1)\n"
     ]
    }
   ],
   "source": [
    "C_values = [100, 50, 25, 10, 1, 0.1]\n",
    "#Kernels = ['rbf', 'poly', 'sigmoid']\n",
    "Kernels = ['rbf', 'poly']\n",
    "\n",
    "best_MSE = 100000\n",
    "best_model = None\n",
    "best_config = (C_values[0], Kernels[0])\n",
    "\n",
    "for kernel in Kernels:\n",
    "    \n",
    "    dev_MSEs = []\n",
    "    train_MSEs = []\n",
    "    \n",
    "    for c in C_values:\n",
    "        \n",
    "        print(kernel, c)\n",
    "        model = sklearn.svm.SVR(C=c, kernel=kernel)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        dev_preds = model.predict(X_dev)\n",
    "        dev_MSE = mean_squared_error(y_dev,dev_preds)\n",
    "        dev_MAE = mean_absolute_error(y_dev, dev_preds)\n",
    "        dev_MSEs.append(dev_MSE)\n",
    "\n",
    "        if dev_MSE<best_MSE:\n",
    "            best_MSE = dev_MSE\n",
    "            best_model = model\n",
    "            best_config = (kernel, c)\n",
    "\n",
    "        print(\"Dev set MSE, MAE\")\n",
    "        print(dev_MSE, dev_MAE)\n",
    "\n",
    "        train_preds = model.predict(X_train)\n",
    "        train_MSE = mean_squared_error(y_train, train_preds)\n",
    "        train_MAE = mean_absolute_error(y_train, train_preds)\n",
    "        train_MSEs.append(train_MSE)\n",
    "\n",
    "        print(\"Train set MSE, MAE\")\n",
    "        print(train_MSE, train_MAE)\n",
    "\n",
    "        print(\"\\n===========\\n\")\n",
    "        \n",
    "    \n",
    "    plt.plot(C_values, dev_MSEs, 'r--', C_values, train_MSEs, 'bs')\n",
    "    plt.xlabel('C')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.legend(['Dev Set', 'Train Set'])\n",
    "    plt.savefig(kernel+\"_svr.jpg\")\n",
    "    plt.close()\n",
    "    \n",
    "# save the model to disk\n",
    "filename = 'E:/Wasif/PDMotorFeatureExtraction/deepmag_svr_fullfit_model.pkl'\n",
    "pickle.dump(best_model, open(filename, 'wb'))\n",
    "print(\"Best Hyperparameters\")\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "impossible-mississippi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26,)\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "prediction = loaded_model.predict(X_test)\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "extraordinary-explanation",
   "metadata": {},
   "source": [
    "i = 0\n",
    "for y_pred in prediction:\n",
    "    print(y_pred, y_test[i])\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "isolated-cooling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance\n",
      "1.0288481014421338 0.6977819268531756\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Performance\")\n",
    "MSE = mean_squared_error(y_test,prediction)\n",
    "MAE = mean_absolute_error(y_test, prediction)\n",
    "print(MSE, MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-accountability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
